{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aehyok/go-openai/blob/main/%E2%80%9Cwhisper_m4a.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "73JHCOCS99hl"
      },
      "source": [
        "ç›´æ¥å°†m4aéŸ³é¢‘æ–‡ä»¶è½¬æ¢ä¸ºå­—å¹•æ–‡ä»¶srt\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mBUm1pmC90Mk",
        "outputId": "edd6feb2-8c38-4164-b4b8-d624952c2b70"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GPU 0: Tesla T4 (UUID: GPU-30297e83-80fd-9354-3339-12897273eb98)\n",
            "Mon Mar 27 11:44:21 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 525.85.12    Driver Version: 525.85.12    CUDA Version: 12.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   45C    P8     9W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "#@markdown # **æ£€æŸ¥GPUç¡¬ä»¶åŠ é€Ÿ** ğŸ•µï¸\n",
        "\n",
        "#@markdown |  GPU   |  GPU RAM   | FP32 teraFLOPS |     Availability   |\n",
        "#@markdown |:------:|:----------:|:--------------:|:------------------:|\n",
        "#@markdown |  T4    |    16 GB   |       8.1      |         Free       |\n",
        "#@markdown | P100   |    16 GB   |      10.6      |      Colab Pro     |\n",
        "#@markdown | V100   |    16 GB   |      15.7      |  Colab Pro (Rare)  |\n",
        "\n",
        "#@markdown ---\n",
        "\n",
        "!nvidia-smi -L\n",
        "\n",
        "!nvidia-smi\n",
        "     "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZMRtV2Lw_BHU",
        "outputId": "5b7ec378-41f6-4147-d34f-496e9f5c4cce"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (2.27.1)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.9/dist-packages (4.11.2)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests) (2.0.12)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests) (2022.12.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests) (3.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests) (1.26.15)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.9/dist-packages (from beautifulsoup4) (2.4)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting git+https://github.com/openai/whisper.git\n",
            "  Cloning https://github.com/openai/whisper.git to /tmp/pip-req-build-6p9at6gc\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/openai/whisper.git /tmp/pip-req-build-6p9at6gc\n",
            "  Resolved https://github.com/openai/whisper.git to commit 6dea21fd7f7253bfe450f1e2512a0fe47ee2d258\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.9/dist-packages (from openai-whisper==20230314) (0.56.4)\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.9/dist-packages (from openai-whisper==20230314) (9.1.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.9/dist-packages (from openai-whisper==20230314) (1.13.1+cu116)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.9/dist-packages (from openai-whisper==20230314) (2.0.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from openai-whisper==20230314) (1.22.4)\n",
            "Requirement already satisfied: tiktoken==0.3.1 in /usr/local/lib/python3.9/dist-packages (from openai-whisper==20230314) (0.3.1)\n",
            "Requirement already satisfied: ffmpeg-python==0.2.0 in /usr/local/lib/python3.9/dist-packages (from openai-whisper==20230314) (0.2.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (from openai-whisper==20230314) (4.65.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.9/dist-packages (from ffmpeg-python==0.2.0->openai-whisper==20230314) (0.18.3)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.9/dist-packages (from tiktoken==0.3.1->openai-whisper==20230314) (2.27.1)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.9/dist-packages (from tiktoken==0.3.1->openai-whisper==20230314) (2022.10.31)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from triton==2.0.0->openai-whisper==20230314) (3.10.1)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.9/dist-packages (from triton==2.0.0->openai-whisper==20230314) (16.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.9/dist-packages (from triton==2.0.0->openai-whisper==20230314) (3.25.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.9/dist-packages (from numba->openai-whisper==20230314) (67.6.0)\n",
            "Requirement already satisfied: llvmlite<0.40,>=0.39.0dev0 in /usr/local/lib/python3.9/dist-packages (from numba->openai-whisper==20230314) (0.39.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from torch->openai-whisper==20230314) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests>=2.26.0->tiktoken==0.3.1->openai-whisper==20230314) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests>=2.26.0->tiktoken==0.3.1->openai-whisper==20230314) (3.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests>=2.26.0->tiktoken==0.3.1->openai-whisper==20230314) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests>=2.26.0->tiktoken==0.3.1->openai-whisper==20230314) (1.26.15)\n",
            "Whisperå·²ç»è¢«å®‰è£…è¯·æ‰§è¡Œä¸‹ä¸€ä¸ªå•å…ƒ\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "æ­£åœ¨ä½¿ç”¨çš„è®¾å¤‡: cuda:0\n"
          ]
        }
      ],
      "source": [
        "#@markdown **é…ç½®Whisper/Setup Whisper** ğŸ—ï¸\n",
        "\n",
        "!pip install requests beautifulsoup4\n",
        "!pip install git+https://github.com/openai/whisper.git\n",
        "\n",
        "import torch\n",
        "import sys\n",
        "\n",
        "device = torch.device('cuda:0')\n",
        "print('æ­£åœ¨ä½¿ç”¨çš„è®¾å¤‡:', device, file=sys.stderr)\n",
        "\n",
        "print('Whisperå·²ç»è¢«å®‰è£…è¯·æ‰§è¡Œä¸‹ä¸€ä¸ªå•å…ƒ')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 46
        },
        "id": "nPV9yHeqARiK",
        "outputId": "0d4e2455-b66e-4e86-eb73-e84c67ba841e"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "**large-v2 modelå·²ç»è¢«é€‰æ‹©äº†**"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# @markdown # ** whisper Modelé€‰æ‹© ** ğŸ§ \n",
        "\n",
        "\n",
        "Model = 'large-v2' #@param ['tiny.en', 'tiny', 'base.en', 'base', 'small.en', 'small', 'medium.en', 'medium', 'large', 'large-v2']\n",
        "#@markdown ---\n",
        "\n",
        "import whisper\n",
        "from IPython.display import Markdown\n",
        "\n",
        "whisper_model = whisper.load_model(Model)\n",
        "\n",
        "if Model in whisper.available_models():\n",
        "    display(Markdown(\n",
        "        f\"**{Model} modelå·²ç»è¢«é€‰æ‹©äº†**\"\n",
        "    ))\n",
        "else:\n",
        "    display(Markdown(\n",
        "        f\"**{Model} model is no longer available.** Please select one of the following: - {' - '.join(whisper.available_models())}\"\n",
        "    ))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "lOnyZpK2EsVe",
        "outputId": "c4ac1e70-4219-4cbc-9450-61e98d91e768"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "audio local path: /content/downloads/test1.m4a\n",
            "audio audio_path: /content/downloads/test1.m4a\n",
            "[00:00.000 --> 00:04.100] ä½ å¥½,æˆ‘æ˜¯å’±ä»¬ä¸“æ çš„å­¦ä¹ å§”å‘˜æœ±è‹±è¾¾\n",
            "[00:04.100 --> 00:06.500] åœ¨é¢„å¤‡çŸ¥è¯†ç‰‡è¿™ä¸ªæ¨¡å—å‘¢\n",
            "[00:06.500 --> 00:08.700] é¾šè€å¸ˆç»™æˆ‘ä»¬ç³»ç»Ÿåœ°æ¢³ç†äº†\n",
            "[00:08.700 --> 00:11.500] ç¼–è¯‘è¿‡ç¨‹ä¸­å„ä¸ªé˜¶æ®µçš„æ ¸å¿ƒè¦ç‚¹\n",
            "[00:11.500 --> 00:12.800] ä¸»è¦çš„ç›®çš„å‘¢\n",
            "[00:12.800 --> 00:16.500] å°±æ˜¯è®©æˆ‘ä»¬å»ºç«‹ä¸€ä¸ªç¼–è¯‘åŸç†çš„çŸ¥è¯†ä½“ç³»\n",
            "[00:16.500 --> 00:18.300] é‚£ä¹ˆåˆ°ä»Šå¤©ä¸ºæ­¢å‘¢\n",
            "[00:18.300 --> 00:20.500] æˆ‘ä»¬å°±å­¦å®Œè¿™éƒ¨åˆ†å†…å®¹äº†\n",
            "[00:20.500 --> 00:23.700] é‚£ä½ å¯¹è¿™äº›çŸ¥è¯†æŒæ¡å¾—å¦‚ä½•å‘¢\n",
            "[00:23.700 --> 00:27.300] ä¸ºäº†å¤ä¹ ,ä¹Ÿä¸ºäº†æ£€æµ‹æˆ‘ä»¬çš„å­¦ä¹ æˆæœ\n",
            "[00:27.500 --> 00:30.600] æˆ‘å°±æ ¹æ®è‡ªå·±çš„ç§¯ç´¯å’Œä¸€äº›å­¦ä¹ ç»éªŒ\n",
            "[00:30.600 --> 00:32.800] æ•´ç†äº†ä¸€å¼ çŸ¥è¯†åœ°å›¾\n",
            "[00:32.800 --> 00:37.100] ä½ å‘¢,å¯ä»¥æ ¹æ®è¿™å¼ åœ°å›¾ä¸­æ ‡è®°çš„ä¸ƒå¤§ç¼–è¯‘é˜¶æ®µ\n",
            "[00:37.100 --> 00:38.800] ç”¨æ¥éšæ—¶é€ŸæŸ¥é‚£äº›\n",
            "[00:38.800 --> 00:42.700] æˆ‘ä»¬ç»å¸¸ä½¿ç”¨çš„ç¼–è¯‘åŸç†æ¦‚å¿µå’Œå…³é”®ç®—æ³•\n",
            "[00:42.700 --> 00:45.700] å¦‚æœä½ ä¹Ÿæ€»ç»“äº†è‡ªå·±çš„çŸ¥è¯†åœ°å›¾å‘¢\n",
            "[00:45.700 --> 00:48.200] é‚£ä½ ä¹Ÿå¯ä»¥å¯¹ç…§ç€æˆ‘æ•´ç†çš„è¿™ä¸ª\n",
            "[00:48.200 --> 00:49.700] ç»™è‡ªå·±ä¸€ä¸ªåé¦ˆ\n",
            "[00:49.700 --> 00:52.000] çœ‹çœ‹å®ƒä»¬ä¹‹é—´æœ‰å“ªäº›ä¸€æ ·\n",
            "[00:52.000 --> 00:53.800] æˆ–è€…æ˜¯ä¸ä¸€æ ·çš„åœ°æ–¹\n",
            "[00:53.800 --> 00:56.700] æˆ‘ä»¬å¯ä»¥åœ¨ç•™è¨€åŒºä¸€èµ·è¿›è¡Œäº¤æµ\n",
            "[00:56.700 --> 00:59.300] ä¸è¿‡å•Š,è™½ç„¶çŸ¥è¯†åœ°å›¾çš„å½¢å¼\n",
            "[00:59.300 --> 01:02.100] æ¯”è¾ƒæ–¹ä¾¿ä½ ä¿å­˜ã€æºå¸¦å’Œé€ŸæŸ¥\n",
            "[01:02.100 --> 01:06.300] ä½†æ˜¯è€ƒè™‘åˆ°çŸ¥è¯†åœ°å›¾é‡Œæ¶‰åŠçš„ä¸€äº›ç†è®ºå•Šã€æ¦‚å¿µå•Š\n",
            "[01:06.300 --> 01:08.400] è¿™äº›çŸ¥è¯†å†…å®¹ä¼šæ¯”è¾ƒå¤š\n",
            "[01:08.400 --> 01:10.700] ä¸æ–¹ä¾¿ä½ è¿›è¡ŒæŸ¥æ‰¾æ£€ç´¢\n",
            "[01:10.700 --> 01:14.600] æ‰€ä»¥,æˆ‘è¿˜ç‰¹æ„æŠŠåœ°å›¾ä¸Šçš„ä¸€äº›çŸ¥è¯†ç‚¹\n",
            "[01:14.600 --> 01:17.700] ç”¨æ–‡å­—çš„å½¢å¼å¸®ä½ è¿›è¡Œäº†æ¢³ç†\n",
            "[01:17.700 --> 01:19.700] ä½ å¯ä»¥å¯¹ç…§ç€æ–‡å­—å†…å®¹\n",
            "[01:19.700 --> 01:23.900] å¤ä¹ å’Œå›é¡¾ç¼–è¯‘æŠ€æœ¯çš„æ ¸å¿ƒæ¦‚å¿µå’Œå…³é”®ç®—æ³•\n",
            "[01:23.900 --> 01:26.700] ç”¨æ¥æ„å»ºå±äºè‡ªå·±çš„çŸ¥è¯†æ¡†æ¶\n",
            "[01:26.700 --> 01:29.900] å¦å¤–,ä½ åœ¨å­¦ä¹ è¿™äº›é¢„å¤‡çŸ¥è¯†çš„è¿‡ç¨‹ä¸­å‘¢\n",
            "[01:29.900 --> 01:31.100] å¯èƒ½ä¼šå‘ç°\n",
            "[01:31.100 --> 01:32.900] é¾šè€å¸ˆå¹¶æ²¡æœ‰éå¸¸æ·±å…¥åœ°\n",
            "[01:32.900 --> 01:36.700] å»è®²è§£è¿™äº›ç¼–è¯‘åŸç†çš„å…·ä½“æ¦‚å¿µã€ç†è®ºå’Œç®—æ³•\n",
            "[01:36.700 --> 01:38.900] æ‰€ä»¥å•Š,å¦‚æœä½ æƒ³ç»§ç»­æ·±å…¥åœ°\n",
            "[01:38.900 --> 01:40.900] å»å­¦ä¹ è¿™äº›åŸºç¡€çŸ¥è¯†\n",
            "[01:40.900 --> 01:43.300] é‚£ä½ å¯ä»¥æ ¹æ®é¾šè€å¸ˆåœ¨æ¯ä¸€è®²æœ€å\n",
            "[01:43.300 --> 01:44.900] åˆ—å‡ºçš„å‚è€ƒèµ„æ–™\n",
            "[01:44.900 --> 01:47.900] åƒé¾™ä¹¦ã€è™ä¹¦ã€ç»ä¹¦ç­‰ç­‰\n",
            "[01:47.900 --> 01:50.700] è¿™äº›æ¯”è¾ƒç»å…¸çš„ç†è®ºä¹¦ç±\n",
            "[01:50.700 --> 01:51.500] å½“ç„¶äº†\n",
            "[01:51.500 --> 01:54.500] ä½ ä¹Ÿå¯ä»¥å»çœ‹çœ‹é¾šè€å¸ˆçš„ç¬¬ä¸€å­£ä¸“æ è¯¾\n",
            "[01:54.500 --> 01:56.300] ç¼–è¯‘åŸç†ä¹‹ç¾\n",
            "[01:56.300 --> 01:57.500] åœ¨æˆ‘çœ‹æ¥å•Š\n",
            "[01:57.500 --> 02:00.500] ç›¸æ¯”ç¼–è¯‘æ–¹é¢çš„ä¸€äº›æ•™ç§‘ä¹¦æ¥è¯´å‘¢\n",
            "[02:00.500 --> 02:03.700] ç¼–è¯‘åŸç†ä¹‹ç¾è¿™é—¨è¯¾ä¼šæ›´åŠ é€šä¿—æ˜“æ‡‚\n",
            "[02:03.700 --> 02:04.900] ä¸æ—¶ä¿±è¿›\n",
            "[02:04.900 --> 02:07.700] å®ƒæ—¢å¯ä»¥ä½œä¸ºä¸€ä¸ªæ–°æ‰‹çš„èµ·æ­¥æŒ‡å¯¼\n",
            "[02:07.700 --> 02:09.100] ä¹Ÿèƒ½å¸®åŠ©ä¸€äº›\n",
            "[02:09.100 --> 02:12.500] æ¯”è¾ƒç†Ÿæ‚‰ç¼–è¯‘æŠ€æœ¯çš„å·¥ç¨‹å¸ˆæ¥æ‰©å±•è§†é‡\n",
            "[02:12.500 --> 02:15.300] æˆ‘å‘¢,å¾ˆæ¨èä½ å»å­¦ä¹ è¿™é—¨è¯¾\n",
            "[02:15.300 --> 02:18.100] æ‰€ä»¥å•Š,æˆ‘é‚€è¯·äº†å’±ä»¬çš„ä¸“æ ç¼–è¾‘\n",
            "[02:18.100 --> 02:19.700] æ·»åŠ äº†ç›¸åº”çš„çŸ¥è¯†ç‚¹\n",
            "[02:19.700 --> 02:22.500] åˆ°ç¼–è¯‘åŸç†ä¹‹ç¾ç›¸åº”æ–‡ç« çš„é“¾æ¥\n",
            "[02:22.500 --> 02:25.100] é‚£å¦‚æœä½ æœ‰æ·±å…¥å­¦ä¹ çš„éœ€è¦çš„è¯\n",
            "[02:25.100 --> 02:27.300] ä½ å°±èƒ½å¾ˆæ–¹ä¾¿çš„æ‰¾åˆ°å®ƒ\n",
            "[02:27.300 --> 02:29.300] å¥½äº†,è¯ä¸å¤šè¯´\n",
            "[02:29.300 --> 02:31.300] å’±ä»¬ä¸€èµ·å¼€å§‹å¤ä¹ å§\n"
          ]
        },
        {
          "data": {
            "text/markdown": [
              "**Transcript file created: test1.srt**"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "#@markdown # **å¼€å§‹è¿è¡Œmodel** ğŸš€\n",
        "\n",
        "#@markdown Run this cell to execute the transcription of the video. This can take a while and very based on the length of the video and the number of parameters of the model selected above.\n",
        "\n",
        "#@markdown ## **Parameters** âš™ï¸\n",
        "\n",
        "#@markdown ### **Behavior control**\n",
        "#@markdown ---\n",
        "language = \"zh\" ## æˆ‘è¿™é‡Œç›´æ¥å†™æ­»ä¸ºä¸­æ–‡çš„\n",
        "#@markdown ---\n",
        "verbose = 'Live transcription' #@param ['Live transcription', 'Progress bar', 'None']\n",
        "#@markdown > Whether to print out the progress and debug messages.\n",
        "#@markdown ---\n",
        "output_format = 'srt' #@param ['txt', 'vtt', 'srt', 'tsv', 'json', 'all']\n",
        "#@markdown > è¦ç”Ÿæˆå“ªç§ç±»å‹çš„å­—å¹•æ–‡ä»¶ï¼Ÿ\n",
        "\n",
        "#@markdown ---\n",
        "task = 'transcribe' #@param ['transcribe', 'translate']\n",
        "#@markdown > Whether to perform X->X speech recognition (`transcribe`) or X->English translation (`translate`).\n",
        "#@markdown ---\n",
        "\n",
        "#@markdown \n",
        "\n",
        "#@markdown ### **Optional: Fine tunning** \n",
        "#@markdown ---\n",
        "temperature = 0.2 #@param {type:\"slider\", min:0, max:1, step:0.05}\n",
        "#@markdown > Temperature to use for sampling.\n",
        "#@markdown ---\n",
        "temperature_increment_on_fallback = 0.2 #@param {type:\"slider\", min:0, max:1, step:0.05}\n",
        "#@markdown > Temperature to increase when falling back when the decoding fails to meet either of the thresholds below.\n",
        "#@markdown ---\n",
        "best_of = 5 #@param {type:\"integer\"}\n",
        "#@markdown > Number of candidates when sampling with non-zero temperature.\n",
        "#@markdown ---\n",
        "beam_size = 8 #@param {type:\"integer\"}\n",
        "#@markdown > Number of beams in beam search, only applicable when temperature is zero.\n",
        "#@markdown ---\n",
        "patience = 1.0 #@param {type:\"number\"}\n",
        "#@markdown > Optional patience value to use in beam decoding, as in [*Beam Decoding with Controlled Patience*](https://arxiv.org/abs/2204.05424), the default (1.0) is equivalent to conventional beam search.\n",
        "#@markdown ---\n",
        "length_penalty = -0.05 #@param {type:\"slider\", min:-0.05, max:1, step:0.05}\n",
        "#@markdown > Optional token length penalty coefficient (alpha) as in [*Google's Neural Machine Translation System*](https://arxiv.org/abs/1609.08144), set to negative value to uses simple length normalization.\n",
        "#@markdown ---\n",
        "suppress_tokens = \"-1\" #@param {type:\"string\"}\n",
        "#@markdown > Comma-separated list of token ids to suppress during sampling; '-1' will suppress most special characters except common punctuations.\n",
        "#@markdown ---\n",
        "initial_prompt = \"\" #@param {type:\"string\"}\n",
        "#@markdown > Optional text to provide as a prompt for the first window.\n",
        "#@markdown ---\n",
        "condition_on_previous_text = True #@param {type:\"boolean\"}\n",
        "#@markdown > if True, provide the previous output of the model as a prompt for the next window; disabling may make the text inconsistent across windows, but the model becomes less prone to getting stuck in a failure loop.\n",
        "#@markdown ---\n",
        "fp16 = True #@param {type:\"boolean\"}\n",
        "#@markdown > whether to perform inference in fp16.\n",
        "#@markdown ---\n",
        "compression_ratio_threshold = 2.4 #@param {type:\"number\"}\n",
        "#@markdown > If the gzip compression ratio is higher than this value, treat the decoding as failed.\n",
        "#@markdown ---\n",
        "logprob_threshold = -1.0 #@param {type:\"number\"}\n",
        "#@markdown > If the average log probability is lower than this value, treat the decoding as failed.\n",
        "#@markdown ---\n",
        "no_speech_threshold = 0.6 #@param {type:\"slider\", min:-0.0, max:1, step:0.05}\n",
        "#@markdown > If the probability of the <|nospeech|> token is higher than this value AND the decoding has failed due to `logprob_threshold`, consider the segment as silence.\n",
        "#@markdown ---\n",
        "\n",
        "verbose_lut = {\n",
        "    'Live transcription': True,\n",
        "    'Progress bar': False,\n",
        "    'None': None\n",
        "}\n",
        "\n",
        "import numpy as np\n",
        "import warnings\n",
        "import shutil\n",
        "from pathlib import Path\n",
        "\n",
        "args = dict(\n",
        "    language = (None if language == \"Auto detection\" else language),\n",
        "    verbose = verbose_lut[verbose],\n",
        "    task = task,\n",
        "    temperature = temperature,\n",
        "    temperature_increment_on_fallback = temperature_increment_on_fallback,\n",
        "    best_of = best_of,\n",
        "    beam_size = beam_size,\n",
        "    patience=patience,\n",
        "    length_penalty=(length_penalty if length_penalty>=0.0 else None),\n",
        "    suppress_tokens=suppress_tokens,\n",
        "    initial_prompt=(None if not initial_prompt else initial_prompt),\n",
        "    condition_on_previous_text=condition_on_previous_text,\n",
        "    fp16=fp16,\n",
        "    compression_ratio_threshold=compression_ratio_threshold,\n",
        "    logprob_threshold=logprob_threshold,\n",
        "    no_speech_threshold=no_speech_threshold\n",
        ")\n",
        "\n",
        "temperature = args.pop(\"temperature\")\n",
        "temperature_increment_on_fallback = args.pop(\"temperature_increment_on_fallback\")\n",
        "if temperature_increment_on_fallback is not None:\n",
        "    temperature = tuple(np.arange(temperature, 1.0 + 1e-6, temperature_increment_on_fallback))\n",
        "else:\n",
        "    temperature = [temperature]\n",
        "\n",
        "if Model.endswith(\".en\") and args[\"language\"] not in {\"en\", \"English\"}:\n",
        "    warnings.warn(f\"{Model} is an English-only model but receipted '{args['language']}'; using English instead.\")\n",
        "    args[\"language\"] = \"en\"\n",
        "title =\"test1\"\n",
        "audio_path = \"/content/downloads/test1.m4a\"\n",
        "audio_path_local = Path(audio_path).resolve()\n",
        "print(\"audio local path:\", audio_path_local)\n",
        "print(\"audio audio_path:\", audio_path)\n",
        "transcription = whisper.transcribe(\n",
        "    whisper_model,\n",
        "    str(audio_path_local),\n",
        "    temperature=temperature,\n",
        "    **args,\n",
        ")\n",
        "\n",
        "# Save output\n",
        "whisper.utils.get_writer(\n",
        "    output_format=output_format,\n",
        "    output_dir=audio_path_local.parent\n",
        ")(\n",
        "    transcription,\n",
        "    title\n",
        ")\n",
        "\n",
        "try:\n",
        "    if output_format==\"all\":\n",
        "        for ext in ('txt', 'vtt', 'srt', 'tsv', 'json'):\n",
        "            transcript_file_name = title + \".\" + ext\n",
        "            display(Markdown(f\"**Transcript file created: {transcript_file_name}**\"))\n",
        "    else:\n",
        "        transcript_file_name = title + \".\" + output_format\n",
        "\n",
        "        display(Markdown(f\"**Transcript file created: {transcript_file_name}**\"))\n",
        "\n",
        "except:\n",
        "    display(Markdown(f\"**Transcript file created: {transcript_local_path}**\"))\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.11.2 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.2"
    },
    "vscode": {
      "interpreter": {
        "hash": "c261aea317cc0286b3b3261fbba9abdec21eaa57589985bb7a274bf54d6cc0a7"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
